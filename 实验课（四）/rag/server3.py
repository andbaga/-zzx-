import os
import asyncio
import nest_asyncio
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel
import uvicorn
import logging
from openai import AsyncOpenAI
from lightrag import LightRAG, QueryParam
from lightrag.llm.hf import hf_embed
from lightrag.utils import EmbeddingFunc
from lightrag.kg.shared_storage import initialize_pipeline_status
from transformers import AutoModel, AutoTokenizer

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("lightrag-server")

nest_asyncio.apply()

app = FastAPI(title="LightRAG Service")

# Enable CORS for frontend interaction
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# Serve frontend
app.mount("/frontend", StaticFiles(directory="frontend"), name="frontend")

@app.get("/")
async def serve_index():
    return FileResponse("frontend/index.html")

# Configuration (Hardcoded for the Medical corpus run)
WORKING_DIR = "./my_rag_storage/Medical"
MODEL_NAME = "deepseek-chat"
# Must match the model used during indexing (bge-large outputs 1024 dim)
EMBED_MODEL_NAME = "BAAI/bge-large-en-v1.5"
LLM_BASE_URL = "https://api.deepseek.com/v1"
LLM_API_KEY = os.getenv("LLM_API_KEY", "sk-ff3b5e94dec5420fba3be260e4ed8d06")

rag_instance = None

# Create OpenAI client for DeepSeek
openai_client = AsyncOpenAI(
    api_key=LLM_API_KEY,
    base_url=LLM_BASE_URL
)

async def llm_model_func(
    prompt: str,
    system_prompt: str = None,
    history_messages: list = [],
    **kwargs
) -> str:
    """Custom LLM function that directly calls DeepSeek API without response_format"""
    # Build messages
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    
    # Add history if any
    for msg in history_messages:
        messages.append(msg)
    
    # Add current prompt
    messages.append({"role": "user", "content": prompt})
    
    try:
        # Call DeepSeek API directly - NO response_format parameter!
        response = await openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=messages,
            temperature=kwargs.get("temperature", 0.0),
            max_tokens=kwargs.get("max_tokens", 4096),
        )
        return response.choices[0].message.content
    except Exception as e:
        logger.error(f"LLM call failed: {e}")
        return ""

async def init_rag():
    global rag_instance
    logger.info(f"Initializing RAG from {WORKING_DIR}...")
    
    tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL_NAME)
    embed_model = AutoModel.from_pretrained(EMBED_MODEL_NAME)
    
    embedding_func = EmbeddingFunc(
        embedding_dim=1024, # Match the dim used during indexing
        max_token_size=8192,
        func=lambda texts: hf_embed(texts, tokenizer, embed_model),
    )
    
    rag_instance = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=llm_model_func,
        llm_model_name=MODEL_NAME,
        embedding_func=embedding_func,
        llm_model_kwargs={}  # Empty - we handle everything in llm_model_func
    )
    await rag_instance.initialize_storages()
    await initialize_pipeline_status()
    logger.info("RAG initialization complete.")

@app.on_event("startup")
async def startup_event():
    await init_rag()

class QueryRequest(BaseModel):
    query: str
    mode: str = "hybrid"

@app.post("/query")
async def query_rag(request: QueryRequest):
    if not rag_instance:
        raise HTTPException(status_code=503, detail="RAG system not initialized")
    
    try:
        param = QueryParam(mode=request.mode)
        response = rag_instance.query(request.query, param=param)
        
        while asyncio.iscoroutine(response):
            response = await response
            
        return {"answer": str(response)}
    except Exception as e:
        logger.error(f"Query failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/status")
async def get_status():
    try:
        return {
            "working_dir": WORKING_DIR,
            "model": MODEL_NAME,
            "embedding_model": EMBED_MODEL_NAME,
            "status": "online"
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.get("/graph-stats")
async def get_graph_stats():    
    async def find_real_count(vdb_instance, name):
        if not vdb_instance:
            return 0
            
        try:
            # ç¬¬ä¸€æ­¥ï¼šè·å–å†…éƒ¨å­˜å‚¨å¯¹è±¡
            # 1. å…ˆæ‰¾ client_storage
            target = getattr(vdb_instance, "client_storage", None)
            
            # 2. å¦‚æœæ˜¯å¼‚æ­¥çš„ï¼Œå¿…é¡» await (è¿™æ˜¯ä¹‹å‰æŠ¥é”™çš„æ ¸å¿ƒåŸå› )
            if asyncio.iscoroutine(target) or hasattr(target, '__await__'):
                target = await target
                
            # 3. å¦‚æœæ²¡æ‰¾åˆ° client_storageï¼Œè¯•è¯• _client
            if target is None:
                target = getattr(vdb_instance, "_client", None)

            # 4. å¦‚æœè¿˜æ˜¯ç©ºçš„ï¼Œç”šè‡³è¯•è¯• _data
            if target is None:
                target = getattr(vdb_instance, "_data", None)

            # ç¬¬äºŒæ­¥ï¼šæš´åŠ›æ‹†è§£ target é‡Œçš„å†…å®¹
            # å¦‚æœ target æœ¬èº«å°±æ˜¯åˆ—è¡¨ï¼Œç›´æ¥è¿”å›
            if isinstance(target, list):
                logger.info(f"{name} æœ¬èº«å°±æ˜¯åˆ—è¡¨ï¼Œé•¿åº¦: {len(target)}")
                return len(target)

            # å¦‚æœ target æ˜¯å­—å…¸ (å°±æ˜¯é‚£ä¸ªé•¿åº¦ä¸º3çš„å®¶ä¼™)
            # æˆ‘ä»¬éå†å®ƒæ‰€æœ‰çš„ Valueï¼Œå¯»æ‰¾é‚£ä¸ªæœ€é•¿çš„åˆ—è¡¨
            if hasattr(target, "__dict__"):
                target = target.__dict__ # æŠŠå¯¹è±¡è½¬æˆå­—å…¸
            
            if isinstance(target, dict):
                # æ‰“å°ä¸€ä¸‹æ‰€æœ‰çš„ Keyï¼Œè®©ä½ å¿ƒé‡Œæœ‰æ•°
                keys = list(target.keys())
                logger.info(f"ğŸ” {name} å†…éƒ¨åŒ…å«è¿™äº› Key: {keys}")
                
                max_len = 0
                
                # æŒ¨ä¸ªæ£€æŸ¥å­—å…¸é‡Œçš„æ¯ä¸€ä¸ªä¸œè¥¿
                for k, v in target.items():
                    # å¦‚æœè¿™ä¸œè¥¿æ˜¯åˆ—è¡¨
                    if isinstance(v, list):
                        curr_len = len(v)
                        logger.info(f"æ£€æŸ¥ Key ['{k}']: æ˜¯åˆ—è¡¨ï¼Œé•¿åº¦ {curr_len}")
                        if curr_len > max_len:
                            max_len = curr_len
                    # å¦‚æœè¿™ä¸œè¥¿æ˜¯å¦ä¸€ä¸ªå¯¹è±¡ï¼Œç”šè‡³å¯èƒ½æœ‰ _data
                    elif hasattr(v, "_data") and isinstance(v._data, list):
                         curr_len = len(v._data)
                         logger.info(f"æ£€æŸ¥ Key ['{k}']._data: æ˜¯åˆ—è¡¨ï¼Œé•¿åº¦ {curr_len}")
                         if curr_len > max_len:
                            max_len = curr_len
                            
                if max_len > 0:
                    logger.info(f"é”å®š {name} çœŸå®é•¿åº¦: {max_len}")
                    return max_len

        except Exception as e:
            logger.error(f"åˆ†æ {name} æ—¶å‡ºé”™: {e}")
        
        return 0

    # å¼€å§‹æ‰§è¡Œæ‰«æ
    entity_count = await find_real_count(getattr(rag_instance, 'entities_vdb', None), "Entities")
    relation_count = await find_real_count(getattr(rag_instance, 'relationships_vdb', None), "Relations")
    chunk_count = await find_real_count(getattr(rag_instance, 'chunks_vdb', None), "Chunks")
    
    return {
        "entities": entity_count,
        "relations": relation_count,
        "chunks": chunk_count,
        "corpus": "Medical",
        "model": MODEL_NAME,
        "embedding_model": EMBED_MODEL_NAME,
        "llm_base_url": LLM_BASE_URL
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)