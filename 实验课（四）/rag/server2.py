import os
import asyncio
import nest_asyncio
import json
import logging
import pandas as pd
import re
import httpx  # éœ€è¦å¯¼å…¥è¿™ä¸ªåº“æ¥æ§åˆ¶ç½‘ç»œè¯·æ±‚
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
from openai import AsyncOpenAI
from lightrag import LightRAG, QueryParam
from lightrag.llm.hf import hf_embed
from lightrag.utils import EmbeddingFunc
from lightrag.kg.shared_storage import initialize_pipeline_status
from transformers import AutoModel, AutoTokenizer

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("lightrag-server")

nest_asyncio.apply()

app = FastAPI(title="LightRAG Service for Evaluation")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== æ ¸å¿ƒé…ç½®åŒºåŸŸ ====================

CORPUS_DIR = r"D:\å¼ æ™ºç‚«çš„æ–‡æ¡£\æ•°æ®æŒ–æ˜ä¸çŸ¥è¯†å¤„ç†\å®éªŒè¯¾ï¼ˆå››ï¼‰\rag\Datasets\Corpus"
DATASET_NAME = os.getenv("DATASET", "novel").lower()
WORKING_DIR = f"./my_rag_storage1/{DATASET_NAME.capitalize()}"

MODEL_NAME = "deepseek-chat"
EMBED_MODEL_NAME = "BAAI/bge-large-en-v1.5"
LLM_BASE_URL = "https://api.deepseek.com/v1"
LLM_API_KEY = os.getenv("LLM_API_KEY", "sk-f2e6433f917d47a7b9a1cc188f65fd70")

# ====================================================

rag_instance = None

# ==================== å…³é”®ä¿®æ”¹ï¼šå¢å¼ºç½‘ç»œç¨³å®šæ€§ ====================
# 1. è®¾ç½®æé•¿çš„è¶…æ—¶æ—¶é—´ (600ç§’ = 10åˆ†é’Ÿ)ï¼Œé˜²æ­¢ DeepSeek å“åº”æ…¢æ—¶æŠ¥é”™
# 2. é™åˆ¶æœ€å¤§è¿æ¥æ•° (max_connections=5)ï¼Œé˜²æ­¢ä¸€æ¬¡å‘å¤ªå¤šè¯·æ±‚æŠŠç½‘ç»œæŒ¤çˆ†
timeout_config = httpx.Timeout(600.0, connect=60.0)
limits_config = httpx.Limits(max_keepalive_connections=5, max_connections=5)

openai_client = AsyncOpenAI(
    api_key=LLM_API_KEY, 
    base_url=LLM_BASE_URL,
    timeout=timeout_config,
    http_client=httpx.AsyncClient(limits=limits_config, timeout=timeout_config)
)
# ================================================================

async def llm_model_func(prompt: str, system_prompt: str = None, history_messages: list = [], **kwargs) -> str:
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    for msg in history_messages:
        messages.append(msg)
    messages.append({"role": "user", "content": prompt})
    
    try:
        # å¢åŠ é‡è¯•é€»è¾‘ï¼Œå¦‚æœå•æ¬¡è°ƒç”¨å¤±è´¥ï¼Œè®© LightRAG çŸ¥é“è¿™ä¸æ˜¯è‡´å‘½é”™è¯¯
        response = await openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=messages,
            temperature=kwargs.get("temperature", 0.0),
            max_tokens=kwargs.get("max_tokens", 4096),
        )
        return response.choices[0].message.content
    except Exception as e:
        logger.error(f"LLM call failed (will retry if possible): {e}")
        raise e 

def split_text_into_chunks(text, max_chars=1000):
    """æ‰‹åŠ¨å°†è¶…é•¿æ–‡æœ¬åˆ‡åˆ†ä¸ºå°æ®µè½"""
    chunks = []
    paragraphs = text.split('\n')
    
    for p in paragraphs:
        p = p.strip()
        if not p:
            continue
        
        if len(p) > max_chars:
            sentences = re.split(r'(?<=[.!?ã€‚ï¼Ÿï¼])\s*', p)
            current_chunk = ""
            for sent in sentences:
                if len(current_chunk) + len(sent) > max_chars:
                    chunks.append(current_chunk)
                    current_chunk = sent
                else:
                    current_chunk += sent
            if current_chunk:
                chunks.append(current_chunk)
        else:
            chunks.append(p)
            
    return chunks

async def ingest_data():
    """è¯»å–æ•°æ®å¹¶æ‰‹åŠ¨é¢„å¤„ç†"""
    logger.info(f"ğŸ“‚ æ­£åœ¨ä»æºç›®å½•æŸ¥æ‰¾æ•°æ®: {CORPUS_DIR}")
    
    json_path = os.path.join(CORPUS_DIR, f"{DATASET_NAME}.json")
    texts_to_insert = []

    if os.path.exists(json_path):
        logger.info(f"ğŸ“– å‘ç° JSON æ–‡ä»¶: {json_path}ï¼Œæ­£åœ¨è¯»å–...")
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            raw_text = ""
            if isinstance(data, list):
                for item in data:
                    raw_text += json.dumps(item, ensure_ascii=False) + "\n"
            elif isinstance(data, dict):
                raw_text = data.get('context') or data.get('text') or json.dumps(data, ensure_ascii=False)
            
            logger.info(f"ğŸ”ª åŸå§‹æ–‡æœ¬è¯»å–å®Œæ¯• (é•¿åº¦ {len(raw_text)} å­—ç¬¦)ï¼Œæ­£åœ¨æ‰‹åŠ¨åˆ‡ç‰‡...")
            
            # æ‰‹åŠ¨åˆ‡ç‰‡
            texts_to_insert = split_text_into_chunks(raw_text, max_chars=2000)
            
            logger.info(f"âœ… åˆ‡ç‰‡å®Œæˆï¼å…±ç”Ÿæˆ {len(texts_to_insert)} ä¸ªå°æ–‡æ¡£ç‰‡æ®µã€‚")

        except Exception as e:
            logger.error(f"âŒ è¯»å– JSON å¤±è´¥: {e}")
            return
    else:
        logger.error(f"âŒ æœªæ‰¾åˆ°æ–‡ä»¶: {json_path}")
        return

    if texts_to_insert:
        logger.info(f"ğŸš€ å‡†å¤‡å°† {len(texts_to_insert)} æ¡æ•°æ®ç‰‡æ®µæ’å…¥ç´¢å¼•...")
        logger.info("â³ æ­£åœ¨ç»§ç»­æ„å»ºçŸ¥è¯†å›¾è°± (å·²å®Œæˆçš„éƒ¨åˆ†ä¼šè‡ªåŠ¨è·³è¿‡)...")
        try:
            await rag_instance.insert(texts_to_insert)
            logger.info("ğŸ‰ğŸ‰ğŸ‰ æ•°æ®ç´¢å¼•æ„å»ºå®Œæˆï¼æ­å–œï¼ ğŸ‰ğŸ‰ğŸ‰")
        except Exception as e:
            logger.error(f"âŒ æ’å…¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    else:
        logger.warning("âš ï¸ æœªæå–åˆ°æœ‰æ•ˆæ•°æ®")

async def init_rag():
    global rag_instance
    logger.info(f"ğŸ‘‰ ç›®æ ‡æ•°æ®é›†: {DATASET_NAME}")
    logger.info(f"ğŸ‘‰ ç´¢å¼•å­˜å‚¨ç›®å½•: {WORKING_DIR}")
    
    if not os.path.exists(WORKING_DIR):
        os.makedirs(WORKING_DIR)

    logger.info("æ­£åœ¨æ£€æŸ¥/åŠ è½½åµŒå…¥æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL_NAME)
    embed_model = AutoModel.from_pretrained(EMBED_MODEL_NAME)
    
    embedding_func = EmbeddingFunc(
        embedding_dim=1024,
        max_token_size=8192,
        func=lambda texts: hf_embed(texts, tokenizer, embed_model),
    )
    
    rag_instance = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=llm_model_func,
        llm_model_name=MODEL_NAME,
        embedding_func=embedding_func,
        chunk_token_size=512,
        llm_model_kwargs={}
    )
    
    await rag_instance.initialize_storages()
    await initialize_pipeline_status()
    await ingest_data()

@app.on_event("startup")
async def startup_event():
    await init_rag()

class QueryRequest(BaseModel):
    query: str
    mode: str = "hybrid"

@app.post("/query")
async def query_rag(request: QueryRequest):
    if not rag_instance:
        raise HTTPException(status_code=503, detail="RAG system not initialized")
    
    try:
        param = QueryParam(mode=request.mode)
        ans_response = rag_instance.query(request.query, param=param)
        
        ctx_param = QueryParam(mode=request.mode, only_need_context=True)
        ctx_response = rag_instance.query(request.query, param=ctx_param)

        while asyncio.iscoroutine(ans_response): ans_response = await ans_response
        while asyncio.iscoroutine(ctx_response): ctx_response = await ctx_response
            
        return {
            "answer": str(ans_response),
            "context": [str(ctx_response)]
        }
    except Exception as e:
        logger.error(f"Query failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/status")
async def get_status():
    return {
        "dataset": DATASET_NAME,
        "working_dir": WORKING_DIR,
        "corpus_source": CORPUS_DIR,
        "status": "ready"
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)