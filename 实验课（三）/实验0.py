import os
import json
import csv
import re
import time
import sys
import torch
import subprocess
from typing import Dict, List, Any, Optional
from transformers import AutoTokenizer, AutoModelForCausalLM

# ===================== å…¨å±€é…ç½®ï¼ˆæ–¹ä¾¿ç”¨æˆ·ä¿®æ”¹ï¼‰=====================
CONFIG = {
    # æ ¸å¿ƒæ¨¡å‹è·¯å¾„ (è¯·ç¡®è®¤æ­¤è·¯å¾„æ­£ç¡®)
    "model_name": r"C:\Users\21002\.cache\huggingface\hub\models--Qwen--Qwen2.5-3B",
    "input_path": r"D:\å¼ æ™ºç‚«çš„æ–‡æ¡£\æ•°æ®æŒ–æ˜ä¸çŸ¥è¯†å¤„ç†\å®éªŒè¯¾ï¼ˆä¸‰ï¼‰\data\raw\Open-Patients.jsonl",
    
    # è¾“å‡ºç›®å½•
    "output_dir": "data/processed",
    "neo4j_dir": os.path.abspath("data/neo4j"), # è·å–ç»å¯¹è·¯å¾„
    
    # Neo4j é…ç½®
    "neo4j_auth": "neo4j/password",
    "neo4j_ports": ("7474", "7687"),
    "docker_image": "neo4j:latest", # ä½¿ç”¨å®˜æ–¹é•œåƒ
    
    # å¤„ç†å‚æ•°
    "max_text_length": 1024,
    "min_entity_types": 2,
    "test_mode": True,
    "test_limit": 4  # æµ‹è¯•æ•°æ®é‡
}

# ===================== æ•°æ®å¤„ç†å·¥å…·ç±» =====================
class MedicalDataProcessor:
    @staticmethod
    def load_jsonl_data(file_path: str) -> List[Dict[str, Any]]:
        data = []
        if not os.path.exists(file_path):
            print(f"é”™è¯¯ï¼šæœªæ‰¾åˆ°åŸå§‹æ•°æ®æ–‡ä»¶ {file_path}")
            return data
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line: continue
                try:
                    data.append(json.loads(line))
                except json.JSONDecodeError:
                    continue
        print(f"æˆåŠŸåŠ è½½ {len(data)} æ¡æœ‰æ•ˆæ•°æ®")
        return data

    @staticmethod
    def save_json_data(data: List[Dict[str, Any]], save_path: str):
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"JSONæ•°æ®å·²ä¿å­˜è‡³ï¼š{save_path}")

    @staticmethod
    def generate_neo4j_csv(data: List[Dict[str, Any]], output_dir: str) -> bool:
        """ç”ŸæˆNeo4jå…¼å®¹çš„èŠ‚ç‚¹/å…³ç³»CSVæ–‡ä»¶"""
        os.makedirs(output_dir, exist_ok=True)
        node_path = os.path.join(output_dir, "nodes.csv")
        rel_path = os.path.join(output_dir, "relationships.csv")

        # ç”ŸæˆèŠ‚ç‚¹CSVï¼ˆå»é‡ï¼‰
        seen_nodes = set()
        with open(node_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['id', 'type', 'name'])  # è¡¨å¤´
            
            entity_mapping = {
                "symptoms": "Symptom",
                "diseases": "Disease",
                "checks": "Check",
                "drugs": "Drug"
            }
            
            for article in data:
                for key, label in entity_mapping.items():
                    for item in article.get(key, []):
                        node_id = f"{key[:-1]}_{item}"  # ç”Ÿæˆå”¯ä¸€IDï¼ˆå¦‚symptom_å’³å—½ï¼‰
                        if node_id not in seen_nodes:
                            writer.writerow([node_id, label, item])
                            seen_nodes.add(node_id)

        # ç”Ÿæˆå…³ç³»CSVï¼ˆå»é‡ï¼‰
        seen_rels = set()
        with open(rel_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['start_id', 'end_id', 'type'])  # è¡¨å¤´
            
            # å®šä¹‰éœ€è¦å»ºç«‹çš„å®ä½“å…³è”
            rel_pairs = [
                ("symptoms", "diseases"),
                ("symptoms", "checks"),
                ("diseases", "checks"),
                ("diseases", "drugs")
            ]
            
            for article in data:
                for start_key, end_key in rel_pairs:
                    start_items = article.get(start_key, [])
                    end_items = article.get(end_key, [])
                    
                    for s_item in start_items:
                        for e_item in end_items:
                            start_id = f"{start_key[:-1]}_{s_item}"
                            end_id = f"{end_key[:-1]}_{e_item}"
                            rel_id = f"{start_id}-{end_id}"
                            
                            if rel_id not in seen_rels:
                                writer.writerow([start_id, end_id, "RELATED_TO"])
                                seen_rels.add(rel_id)

        print(f"Neo4j CSVæ–‡ä»¶å·²ç”Ÿæˆè‡³ï¼š{output_dir}")
        return True

# ===================== ç¿»è¯‘å·¥å…· =====================
def translate_en_to_zh(text: str, model: AutoModelForCausalLM, tokenizer: AutoTokenizer, device: torch.device) -> str:
    """è‹±æ–‡åŒ»å­¦æ–‡æœ¬è½¬ä¸­æ–‡ï¼ˆä¿ç•™åŸç¿»è¯‘é€»è¾‘ï¼Œä¼˜åŒ–æç¤ºè¯è¡¨è¿°ï¼‰"""
    if not text or not isinstance(text, str) or text.strip() == "":
        print("è­¦å‘Šï¼šæ— æ•ˆçš„ç¿»è¯‘è¾“å…¥æ–‡æœ¬")
        return text.strip() if text else ""
    
    # ä¼˜åŒ–åçš„ç¿»è¯‘æç¤ºè¯ï¼ˆæ˜ç¡®è¦æ±‚ç®€æ´å‡†ç¡®ï¼‰
    prompt = f"""è¯·å°†ä»¥ä¸‹è‹±æ–‡åŒ»å­¦æ–‡æœ¬ç²¾å‡†ç¿»è¯‘æˆä¸­æ–‡ï¼Œä»…è¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸æ·»åŠ ä»»ä½•è§£é‡Šã€æé—®æˆ–é¢å¤–å†…å®¹ï¼š
è‹±æ–‡åŸæ–‡ï¼š{text.strip()}
ä¸­æ–‡ç¿»è¯‘ï¼š"""
    
    try:
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=CONFIG["max_text_length"]).to(device)
        
        generation_kwargs = {
            "max_new_tokens": 1024,
            "do_sample": False,
            "num_beams": 1,
            "repetition_penalty": 1.1,
            "pad_token_id": tokenizer.eos_token_id,
            "eos_token_id": tokenizer.eos_token_id
        }
        
        with torch.no_grad():
            outputs = model.generate(**inputs, **generation_kwargs)
        
        # æå–å¹¶æ¸…ç†ç¿»è¯‘ç»“æœ
        translated = tokenizer.decode(outputs[0], skip_special_tokens=True)
        translated = translated.replace(prompt, "").strip().split('\n')[0]
        return translated if translated else text.strip()
    
    except Exception as e:
        print(f"è­¦å‘Šï¼šç¿»è¯‘å¤±è´¥ - {str(e)}ï¼Œè¿”å›åŸæ–‡")
        return text.strip()

# ===================== åŒ»å­¦å®ä½“æå–å™¨ =====================
class MedicalEntityExtractor:
    """æ•´åˆæ¨¡å‹åŠ è½½ä¸å®ä½“æå–åŠŸèƒ½ï¼ˆä¼˜åŒ–é”™è¯¯å¤„ç†ï¼‰"""
    def __init__(self, model_name: str, device: Optional[torch.device] = None):
        self.device = self._get_device(device)
        self.tokenizer = self._load_tokenizer(model_name)
        self.model = self._load_model(model_name)

    def _get_device(self, device: Optional[torch.device]) -> torch.device:
        """è‡ªåŠ¨é€‰æ‹©è®¡ç®—è®¾å¤‡ï¼ˆGPUä¼˜å…ˆï¼‰"""
        if device:
            return device
        return torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def _load_tokenizer(self, model_name: str) -> AutoTokenizer:
        """åŠ è½½åˆ†è¯å™¨ï¼ˆå¤„ç†pad_tokenï¼‰"""
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.pad_token_id = tokenizer.eos_token_id
        print("åˆ†è¯å™¨åŠ è½½å®Œæˆ")
        return tokenizer

    def _load_model(self, model_name: str) -> AutoModelForCausalLM:
        """åŠ è½½æ¨¡å‹ï¼ˆä¼˜åŒ–å†…å­˜é…ç½®ï¼‰"""
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹ {model_name}ï¼ˆè®¾å¤‡ï¼š{self.device}ï¼‰")
        model_kwargs = {
            "trust_remote_code": True,
            "device_map": "auto",
            "low_cpu_mem_usage": True
        }
        
        # æ ¹æ®è®¾å¤‡è®¾ç½®ç²¾åº¦
        if self.device.type == "cpu":
            model_kwargs["torch_dtype"] = torch.float32
        else:
            model_kwargs["torch_dtype"] = torch.float16

        try:
            model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)
            return model.to(self.device)
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°è¯•ä¿å®ˆé…ç½® - {str(e)}")
            model_kwargs["torch_dtype"] = torch.float32
            return AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs).to(self.device)

    def _clean_json_str(self, json_str: str) -> str:
        """æ¸…ç†JSONå­—ç¬¦ä¸²ï¼ˆä¿®å¤æ ¼å¼é”™è¯¯ï¼‰"""
        # æ›¿æ¢å•å¼•å·ã€å»é™¤å°¾éƒ¨é€—å·ã€ä¿®å¤é”®åå¼•å·
        json_str = json_str.replace("'", '"').replace(",]", "]").replace(",}", "}")
        json_str = re.sub(r'([{,])\s*([a-zA-Z0-9_]+):', r'\1"\2":', json_str)
        # æå–ç¬¬ä¸€ä¸ªå®Œæ•´JSONå¯¹è±¡
        json_matches = re.findall(r'\{[^{}]*\}', json_str, re.DOTALL)
        if json_matches:
            json_str = json_matches[0]
        return json_str.strip()

    def _extract_manual(self, text: str) -> Optional[Dict[str, List[str]]]:
        """JSONè§£æå¤±è´¥æ—¶æ‰‹åŠ¨æå–å®ä½“ï¼ˆä¼˜åŒ–æ­£åˆ™åŒ¹é…ï¼‰"""
        result = {"symptoms": [], "diseases": [], "checks": [], "drugs": []}
        entity_types = list(result.keys())
        
        for et in entity_types:
            # å¤šæ¨¡å¼åŒ¹é…å®ä½“åˆ—è¡¨
            patterns = [
                rf'"{et}"\s*:\s*\[(.*?)\]',
                rf'{et}\s*:\s*\[(.*?)\]',
                rf'{et}\s*=\s*\[(.*?)\]'
            ]
            
            for pat in patterns:
                matches = re.findall(pat, text, re.DOTALL | re.IGNORECASE)
                for match in matches:
                    if not match.strip():
                        continue
                    # æå–å¼•å·å†…æˆ–é€—å·åˆ†éš”çš„å®ä½“
                    items = re.findall(r'"([^"]*)"', match) or re.findall(r"'([^']*)'", match)
                    if not items:
                        items = [i.strip() for i in match.split(',') if i.strip()]
                    result[et].extend([i for i in items if i])
        
        # å»é‡å¹¶è¿‡æ»¤ç©ºå€¼
        for et in entity_types:
            result[et] = list(set([i for i in result[et] if i]))
        
        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æœ€å°å®ä½“ç±»å‹æ•°
        valid_types = sum(1 for v in result.values() if v)
        return result if valid_types >= CONFIG["min_entity_types"] else None

    def extract(self, text: str) -> Optional[Dict[str, List[str]]]:
        """æ ¸å¿ƒå®ä½“æå–æ–¹æ³•ï¼ˆæ•´åˆJSONè§£æä¸æ‰‹åŠ¨æå–ï¼‰"""
        # æ–‡æœ¬æˆªæ–­
        if len(text) > CONFIG["max_text_length"]:
            text = text[:CONFIG["max_text_length"]]
            print(f"æç¤ºï¼šæ–‡æœ¬è¿‡é•¿ï¼Œå·²æˆªæ–­è‡³ {CONFIG['max_text_length']} å­—ç¬¦")
        
        # ä¼˜åŒ–åçš„å®ä½“æå–æç¤ºè¯
        prompt = f"""è¯·ä»ä»¥ä¸‹åŒ»å­¦ç—…ä¾‹ä¸­æå–4ç±»å®ä½“ï¼šç—‡çŠ¶ï¼ˆsymptomsï¼‰ã€ç–¾ç—…ï¼ˆdiseasesï¼‰ã€æ£€æŸ¥ï¼ˆchecksï¼‰ã€è¯ç‰©ï¼ˆdrugsï¼‰ã€‚
è¦æ±‚ï¼š
1. æ— å¯¹åº”å®ä½“åˆ™è¿”å›ç©ºæ•°ç»„
2. ä»…è¾“å‡ºJSONæ ¼å¼ï¼Œæ— ä»»ä½•é¢å¤–å†…å®¹
3. JSONé”®åå¿…é¡»ä¸ºæŒ‡å®šè‹±æ–‡ï¼ˆsymptoms/diseases/checks/drugsï¼‰
4. æ•°ç»„é¡¹ç”¨åŒå¼•å·åŒ…è£¹ï¼Œé€—å·åˆ†éš”

ç—…ä¾‹æ–‡æœ¬ï¼š{text}

JSONè¾“å‡ºï¼š"""
        
        try:
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=CONFIG["max_text_length"]).to(self.device)
            generation_kwargs = {
                "max_new_tokens": 512,
                "do_sample": True,
                "temperature": 0.1,
                "repetition_penalty": 1.1,
                "pad_token_id": self.tokenizer.eos_token_id
            }
            
            with torch.no_grad():
                outputs = self.model.generate(**inputs, **generation_kwargs)
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # æå–JSONéƒ¨åˆ†
            json_str = self._clean_json_str(response)
            result = json.loads(json_str)
            
            # éªŒè¯æ ¼å¼å¹¶å»é‡
            for et in ["symptoms", "diseases", "checks", "drugs"]:
                if et not in result:
                    result[et] = []
                result[et] = list(set([i.strip() for i in result[et] if i.strip()]))
            
            # æ£€æŸ¥æœ€å°å®ä½“ç±»å‹æ•°
            valid_types = sum(1 for v in result.values() if v)
            if valid_types >= CONFIG["min_entity_types"]:
                return result
            else:
                print(f"æç¤ºï¼šä»…æå–åˆ° {valid_types} ç±»å®ä½“ï¼Œä½äºæœ€å°è¦æ±‚")
                return None
        
        except json.JSONDecodeError:
            print("è­¦å‘Šï¼šJSONè§£æå¤±è´¥ï¼Œå°è¯•æ‰‹åŠ¨æå–")
            return self._extract_manual(response)
        except Exception as e:
            print(f"é”™è¯¯ï¼šå®ä½“æå–å¤±è´¥ - {str(e)}")
            return None

# ===================== Neo4jè‡ªåŠ¨åŒ–å·¥å…· =====================
class Neo4jAutoDeploy:
    """æ•´åˆNeo4jå®¹å™¨å¯åŠ¨ã€æ–‡ä»¶æ‹·è´ã€æ•°æ®å¯¼å…¥ï¼ˆå«æ™ºèƒ½é‡è¯•æœºåˆ¶ï¼‰"""
    
    @staticmethod
    def run_docker_cmd(cmd: List[str], check_error=True, suppress_output=False) -> bool:
        """æ‰§è¡ŒDockerå‘½ä»¤"""
        try:
            if not suppress_output:
                print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            result = subprocess.run(cmd, check=check_error, capture_output=True, text=True)
            return True
        except subprocess.CalledProcessError as e:
            if check_error and not suppress_output:
                print(f"âŒ å‘½ä»¤æ‰§è¡Œå¤±è´¥: {e.stderr.strip()}")
            return False

    def start_container(self):
        """å¯åŠ¨Neo4jå®¹å™¨"""
        image_name = "neo4j:latest" 
        
        # 1. å¼ºåˆ¶æ¸…ç†æ—§å®¹å™¨ (ç¡®ä¿ç¯å¢ƒå¹²å‡€)
        print("æ¸…ç†æ—§å®¹å™¨ç¯å¢ƒ...")
        self.run_docker_cmd(["docker", "stop", "neo4j"], check_error=False, suppress_output=True)
        self.run_docker_cmd(["docker", "rm", "neo4j"], check_error=False, suppress_output=True)
        
        # 2. å¯åŠ¨æ–°å®¹å™¨
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åªæŒ‚è½½ data ç›®å½•åšæŒä¹…åŒ–ï¼Œimport ç›®å½•æˆ‘ä»¬åé¢ç”¨ docker cp æ¨¡æ‹Ÿæ‰‹åŠ¨æ“ä½œ
        # è¿™æ ·å¯ä»¥é¿å… Windows æŒ‚è½½å¯¼è‡´çš„æ–‡ä»¶è¯»å–æƒé™é—®é¢˜
        abs_neo4j_dir = os.path.abspath(CONFIG['neo4j_dir'])
        ports = CONFIG["neo4j_ports"]
        
        run_cmd = [
            "docker", "run", "--name", "neo4j",
            "-p", f"{ports[0]}:{ports[0]}", "-p", f"{ports[1]}:{ports[1]}",
            "-v", f"{abs_neo4j_dir}:/data", 
            "-d", "-e", f"NEO4J_AUTH={CONFIG['neo4j_auth']}",
            image_name
        ]
        
        print(f"ğŸš€ æ­£åœ¨å¯åŠ¨å®¹å™¨ {image_name}...")
        if self.run_docker_cmd(run_cmd):
            print("âœ… å®¹å™¨å¯åŠ¨æŒ‡ä»¤å·²å‘é€")
            return True
        return False

    def wait_for_neo4j_and_import(self, csv_dir: str):
        """æ ¸å¿ƒé€»è¾‘ï¼šç­‰å¾…æ•°æ®åº“å°±ç»ª -> æ‹·è´æ–‡ä»¶ -> å¯¼å…¥æ•°æ®"""
        
        # 1. æ¨¡æ‹Ÿ docker cp æ“ä½œ
        print("\n[è‡ªåŠ¨åŒ–] æ­£åœ¨å°†CSVæ–‡ä»¶æ‹·è´è‡³å®¹å™¨å†…éƒ¨...")
        for csv_file in ["nodes.csv", "relationships.csv"]:
            src_path = os.path.join(csv_dir, csv_file)
            if not os.path.exists(src_path):
                print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {src_path}")
                return False
            
            # ä½¿ç”¨ docker cp å‘½ä»¤
            cp_cmd = ["docker", "cp", src_path, "neo4j:/var/lib/neo4j/import/"]
            if not self.run_docker_cmd(cp_cmd):
                print(f"âŒ æ–‡ä»¶ {csv_file} æ‹·è´å¤±è´¥")
                return False
        print("âœ… æ–‡ä»¶æ‹·è´å®Œæˆ")

        # 2. å‡†å¤‡å¯¼å…¥å‘½ä»¤
        # ä½ çš„æ‰‹åŠ¨å‘½ä»¤éå¸¸å®Œç¾ï¼Œæˆ‘ä»¬ç›´æ¥å¤ç”¨å®ƒ
        cypher_query = (
            "LOAD CSV WITH HEADERS FROM 'file:///nodes.csv' AS row "
            "CREATE (n:MedicalEntity {id: row.id, type: row.type, name: row.name}); "
            "LOAD CSV WITH HEADERS FROM 'file:///relationships.csv' AS row "
            "MATCH (start:MedicalEntity {id: row.start_id}) "
            "MATCH (end:MedicalEntity {id: row.end_id}) "
            "CREATE (start)-[r:RELATED_TO]->(end);"
        )
        
        import_cmd = [
            "docker", "exec", "neo4j", "cypher-shell",
            "-u", CONFIG["neo4j_auth"].split('/')[0],
            "-p", CONFIG["neo4j_auth"].split('/')[1],
            cypher_query
        ]

        # 3. å¾ªç¯é‡è¯•æœºåˆ¶ (ä¸“é—¨è§£å†³ Connection refused)
        print("\n[è‡ªåŠ¨åŒ–] å¼€å§‹å°è¯•è¿æ¥æ•°æ®åº“å¹¶å¯¼å…¥ (æœ€å¤šç­‰å¾… 5 åˆ†é’Ÿ)...")
        max_retries = 30
        for i in range(1, max_retries + 1):
            sys.stdout.write(f"\râ³ ç¬¬ {i}/{max_retries} æ¬¡å°è¯•è¿æ¥ Neo4j... ")
            sys.stdout.flush()
            
            try:
                # å°è¯•æ‰§è¡Œå¯¼å…¥
                result = subprocess.run(import_cmd, check=True, capture_output=True, text=True)
                print("\n\nğŸ‰ å¯¼å…¥æˆåŠŸï¼(Exit Code: 0)")
                return True
            except subprocess.CalledProcessError as e:
                error_msg = e.stderr.lower()
                # å¦‚æœæ˜¯è¿æ¥é”™è¯¯ï¼Œè¯´æ˜è¿˜åœ¨å¯åŠ¨ä¸­ï¼Œç­‰å¾…å¹¶é‡è¯•
                if "connection refused" in error_msg or "failed to connect" in error_msg or "connect to localhost" in error_msg:
                    time.sleep(10) # ç­‰å¾…10ç§’å†è¯•
                else:
                    # å¦‚æœæ˜¯å…¶ä»–é”™è¯¯ï¼ˆæ¯”å¦‚è¯­æ³•é”™è¯¯ï¼‰ï¼Œç›´æ¥æŠ¥é”™åœæ­¢
                    print(f"\nâŒ å‘ç”Ÿéè¿æ¥é”™è¯¯ï¼Œåœæ­¢é‡è¯•:\n{e.stderr}")
                    return False
        
        print("\nâŒ è¶…æ—¶ï¼šNeo4j å¯åŠ¨æ—¶é—´è¿‡é•¿ï¼Œè¯·æ£€æŸ¥ Docker æ—¥å¿—ã€‚")
        return False

# ===================== ä¸»æµç¨‹ =====================
def main():
    print("="*60)
    print("          åŒ»å­¦å®ä½“çŸ¥è¯†å›¾è°±æ„å»ºæµç¨‹å¯åŠ¨          ")
    print("="*60)

    # 1. åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
    print("\nã€æ­¥éª¤1/5ã€‘åˆå§‹åŒ–æ¨¡å‹ä¸å·¥å…·...")
    extractor = MedicalEntityExtractor(CONFIG["model_name"])
    data_processor = MedicalDataProcessor()
    neo4j_deployer = Neo4jAutoDeploy()

    # 2. åŠ è½½åŸå§‹æ•°æ®
    print("\nã€æ­¥éª¤2/5ã€‘åŠ è½½åŸå§‹æ•°æ®...")
    raw_data = data_processor.load_jsonl_data(CONFIG["input_path"])
    if not raw_data:
        print("é”™è¯¯ï¼šæ— æœ‰æ•ˆåŸå§‹æ•°æ®ï¼Œæµç¨‹ç»ˆæ­¢")
        return

    # 3. å¤„ç†æ•°æ®ï¼ˆç¿»è¯‘+å®ä½“æå–ï¼‰
    print("\nã€æ­¥éª¤3/5ã€‘å¤„ç†æ•°æ®ï¼ˆç¿»è¯‘+å®ä½“æå–ï¼‰...")
    processed_data = []
    # é€»è¾‘ï¼šå¦‚æœæ˜¯æµ‹è¯•æ¨¡å¼ï¼Œåªå–å‰Næ¡ï¼›å¦åˆ™å…¨é‡
    limit = CONFIG["test_limit"] if CONFIG["test_mode"] else len(raw_data)
    target_data = raw_data[:limit]
    
    for i, item in enumerate(target_data, 1):
        print(f"\n--- å¤„ç†è¿›åº¦ {i}/{len(target_data)} ---")
        
        original_text = item.get("description", "")
        if not original_text:
            continue
        
        # ç¿»è¯‘
        translated = translate_en_to_zh(original_text, extractor.model, extractor.tokenizer, extractor.device)
        
        # æå–å®ä½“
        entities = extractor.extract(translated)
        if entities:
            print(f"  æˆåŠŸæå–å®ä½“: {sum(len(v) for v in entities.values())} ä¸ª")
            processed_item = {
                "id": item.get("_id", f"item_{i}"),
                "original": original_text,
                "translated": translated,
                **entities
            }
            processed_data.append(processed_item)
        else:
            print("  æœªæå–åˆ°æœ‰æ•ˆå®ä½“")

    if not processed_data:
        print("âŒ é”™è¯¯ï¼šæ— æœ‰æ•ˆå¤„ç†ç»“æœï¼Œæµç¨‹ç»ˆæ­¢")
        return

    # 4. ä¿å­˜ç»“æœ
    print("\nã€æ­¥éª¤4/5ã€‘ä¿å­˜å¤„ç†ç»“æœ...")
    json_path = os.path.join(CONFIG["output_dir"], "processed_articles.json")
    data_processor.save_json_data(processed_data, json_path)
    
    neo4j_csv_dir = os.path.join(CONFIG["output_dir"], "neo4j")
    data_processor.generate_neo4j_csv(processed_data, neo4j_csv_dir)

    # 5. Neo4j å…¨è‡ªåŠ¨éƒ¨ç½²
    print("\nã€æ­¥éª¤5/5ã€‘Neo4j è‡ªåŠ¨åŒ–éƒ¨ç½²ä¸å¯¼å…¥...")
    
    # ç¬¬ä¸€æ­¥ï¼šå¯åŠ¨å®¹å™¨
    if neo4j_deployer.start_container():
        # ç¬¬äºŒæ­¥ï¼šæ™ºèƒ½ç­‰å¾…å¹¶å¯¼å…¥ï¼ˆæ•´åˆäº†cpå’Œloopé€»è¾‘ï¼‰
        if neo4j_deployer.wait_for_neo4j_and_import(neo4j_csv_dir):
            print("\n" + "="*60)
            print("ğŸ‰ æ­å–œï¼å…¨æµç¨‹æ‰§è¡ŒæˆåŠŸï¼")
            print(f"ğŸ‘‰ çŸ¥è¯†å›¾è°±æŸ¥çœ‹åœ°å€: http://localhost:{CONFIG['neo4j_ports'][0]}")
            print(f"ğŸ‘‰ ç™»å½•è´¦å·: {CONFIG['neo4j_auth'].split('/')[0]}")
            print(f"ğŸ‘‰ ç™»å½•å¯†ç : {CONFIG['neo4j_auth'].split('/')[1]}")
            print("="*60)
        else:
            print("âŒ å¯¼å…¥é˜¶æ®µå¤±è´¥")
    else:
        print("âŒ å®¹å™¨å¯åŠ¨å¤±è´¥")

if __name__ == "__main__":
    main()